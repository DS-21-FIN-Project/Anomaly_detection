{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 라벨 비율 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0: 27159 objects, 76.41% of total\n",
      "Class 99: 816 objects, 2.30% of total\n",
      "Class 2: 6866 objects, 19.32% of total\n",
      "Class 1: 702 objects, 1.98% of total\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# 라벨이 저장된 폴더 경로 설정\n",
    "label_folder_path = 'falldown/train'\n",
    "\n",
    "# 클래스별 개수를 저장할 딕셔너리\n",
    "class_counts = {}\n",
    "\n",
    "# 지정된 폴더 내의 모든 txt 파일을 읽음\n",
    "for label_file in glob.glob(os.path.join(label_folder_path, '*.txt')):\n",
    "    with open(label_file, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            class_id = line.split()[0]  # 클래스 ID 추출\n",
    "            if class_id in class_counts:\n",
    "                class_counts[class_id] += 1\n",
    "            else:\n",
    "                class_counts[class_id] = 1\n",
    "\n",
    "# 총 개체 수 계산\n",
    "total_count = sum(class_counts.values())\n",
    "\n",
    "# 각 클래스의 비율 계산 및 출력\n",
    "for class_id, count in class_counts.items():\n",
    "    print(f'Class {class_id}: {count} objects, {count / total_count * 100:.2f}% of total')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 커널 충돌 방지 코드\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComplexPedestrianDataset(Dataset):\n",
    "    def __init__(self, folder_path, transform=None):\n",
    "        self.folder_path = folder_path\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "\n",
    "        # 이미지 파일을 탐색하고, 각 이미지에 대한 라벨 정보를 로드합니다.\n",
    "        for filename in os.listdir(folder_path):\n",
    "            if filename.endswith('.jpg'):\n",
    "                image_path = os.path.join(folder_path, filename)\n",
    "                label_path = image_path.replace('.jpg', '.txt')  # 라벨 파일 경로\n",
    "\n",
    "                if os.path.exists(label_path):\n",
    "                    # 라벨 파일이 존재하면, 라벨 데이터를 파싱합니다.\n",
    "                    with open(label_path, 'r') as f:\n",
    "                        labels = [line.strip().split() for line in f.readlines()]\n",
    "                        labels = [[int(label[0])] + [float(x) for x in label[1:]] for label in labels]\n",
    "                        self.image_paths.append(image_path)\n",
    "                        self.labels.append(labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        labels = self.labels[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        # 이미지와 함께 객체별 라벨 정보를 반환합니다.\n",
    "        return image, labels\n",
    "\n",
    "\n",
    "# 데이터 증강 작업을 수행함\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "folder_path = 'falldown/train'\n",
    "dataset = ComplexPedestrianDataset(folder_path, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)  # 배치 크기는 상황에 따라 조정\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class ImageFeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImageFeatureExtractor, self).__init__()\n",
    "        # Pretrained CNN 모델을 사용하여 이미지 특징을 추출합니다.\n",
    "        # 예를 들어, ResNet50을 사용할 수 있으며, 마지막 분류층을 제외합니다.\n",
    "        self.feature_extractor = models.resnet50(pretrained=True)\n",
    "        self.feature_extractor = nn.Sequential(*list(self.feature_extractor.children())[:-2])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # CNN을 통과한 이미지 특징 벡터를 반환합니다.\n",
    "        return self.feature_extractor(x)\n",
    "\n",
    "class LSTMForObjectDetection(nn.Module):\n",
    "    def __init__(self, num_classes, hidden_size=256):\n",
    "        super(LSTMForObjectDetection, self).__init__()\n",
    "        self.image_feature_extractor = ImageFeatureExtractor()\n",
    "        self.lstm = nn.LSTM(input_size=2048, hidden_size=hidden_size, batch_first=True)\n",
    "        # 분류를 위한 출력층: 각 클래스에 대한 예측 확률\n",
    "        self.fc_class = nn.Linear(hidden_size, num_classes)\n",
    "        # 위치 예측을 위한 출력층: 중심 좌표(x, y), 너비(w), 높이(h)\n",
    "        self.fc_bbox = nn.Linear(hidden_size, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        # 이미지에서 특징 추출\n",
    "        features = self.image_feature_extractor(x)  # (batch_size, C, H, W)\n",
    "        # 특징 벡터를 LSTM의 입력 형식에 맞게 변환\n",
    "        features = features.permute(0, 2, 3, 1)  # (batch_size, H, W, C)\n",
    "        features = features.contiguous().view(batch_size, -1, features.size(-1))  # (batch_size, H*W, C)\n",
    "\n",
    "        # LSTM을 통해 시퀀스 데이터 처리\n",
    "        lstm_out, _ = self.lstm(features)\n",
    "        # 최종 시퀀스 출력을 분류 및 위치 예측에 사용\n",
    "        class_outputs = self.fc_class(lstm_out[:, -1, :])  # 마지막 시퀀스 아이템 사용\n",
    "        bbox_outputs = self.fc_bbox(lstm_out[:, -1, :])\n",
    "        \n",
    "        return class_outputs, bbox_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# 모델, 데이터셋 클래스, 그리고 다른 필요한 정의들이 이미 완료되었다고 가정합니다.\n",
    "\n",
    "# 파라미터 설정\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "num_classes = 4  # 0-사람, 1-오인 , 2-실신, 3-폭행, 99-마스킹\n",
    "hidden_size = 256\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# collate_fn 함수 정의\n",
    "def collate_fn(batch):\n",
    "    images, targets = list(zip(*batch))\n",
    "    # 이미지는 일반적으로 transform을 통해 동일한 크기로 조정되므로 바로 스택을 사용할 수 있습니다.\n",
    "    images = torch.stack(images, dim=0).to(device)\n",
    "\n",
    "    # 타겟 처리\n",
    "    # 가장 긴 타겟을 찾습니다.\n",
    "    max_len = max(len(t) for t in targets)\n",
    "    # 모든 타겟이 동일한 길이를 갖도록 패딩을 추가합니다.\n",
    "    padded_targets = []\n",
    "    for t in targets:\n",
    "        padded = t + [[-1, -1, -1, -1, -1]] * (max_len - len(t))  # 예시 패딩 값으로는 -1을 사용\n",
    "        padded_targets.append(padded)\n",
    "\n",
    "    targets_padded = torch.tensor(padded_targets, dtype=torch.float).to(device) \n",
    "    return images, targets_padded\n",
    "\n",
    "# 데이터셋 및 데이터 로더 초기화\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "train_dataset = ComplexPedestrianDataset(folder_path='falldown/train', transform=transform)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# 모델 초기화\n",
    "model = LSTMForObjectDetection(num_classes=num_classes, hidden_size=hidden_size).to(device)\n",
    "\n",
    "# 손실 함수와 옵티마이저\n",
    "criterion_class = torch.nn.CrossEntropyLoss()  # 분류 손실\n",
    "criterion_bbox = torch.nn.MSELoss()  # 회귀 손실(바운딩 박스)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.ComplexPedestrianDataset at 0x17b52819f90>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x17b5281a2f0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 학습 루프 수정\n",
    "for epoch in range(num_epochs):\n",
    "    for images, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 모델 예측\n",
    "        class_outputs, bbox_outputs = model(images)\n",
    "\n",
    "        # targets 내용을 확인하고 적절히 처리\n",
    "        class_labels = []\n",
    "        bbox_labels = []\n",
    "        for batch_item  in targets:\n",
    "            # 여기서 각 target은 [class_label, x_center, y_center, width, height]의 형태를 가짐\n",
    "            for obj in batch_item:\n",
    "                class_label, bbox = obj[0], obj[1:]\n",
    "                class_labels.append(class_label)\n",
    "                bbox_labels.extend(bbox) \n",
    "\n",
    "        # 이제 class_labels와 bbox_labels를 PyTorch 텐서로 변환할 수 있습니다.\n",
    "        class_labels_tensor = torch.tensor(class_labels, dtype=torch.long).to(images.device)\n",
    "        bbox_labels_tensor = torch.tensor(bbox_labels, dtype=torch.float).to(images.device).view(-1, 4)\n",
    "\n",
    "        # print(class_labels_tensor.size())\n",
    "        # print(bbox_labels_tensor.size())\n",
    "\n",
    "        # 손실 계산\n",
    "        loss_class = criterion_class(class_outputs, class_labels_tensor)\n",
    "        loss_bbox = criterion_bbox(bbox_outputs, bbox_labels_tensor)\n",
    "        total_loss = loss_class + loss_bbox\n",
    "\n",
    "\n",
    "        # 역전파 및 옵티마이저 스텝\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
